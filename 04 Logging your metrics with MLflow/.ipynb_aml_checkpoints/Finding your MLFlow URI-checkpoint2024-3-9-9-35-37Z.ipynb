{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning and saving to the model registry\n",
        "\n",
        "In this notebook, you extend on your previous work to actually finetune a model and save it in the Azure ML Registry.\n",
        "To do so, you will:\n",
        "\n",
        "1. Find your MLFlow URI \n",
        "2. Modify the Axolotl YML file to point to your MLFlow URI\n",
        "3. Submit your finetuning job and monitor it\n",
        "\n",
        "## Goal\n",
        "\n",
        "The configuration of MLFlow is quite simple: with Axolotl, you need to simplyl point out to an MLFlow URI, which is specific to your workspace.\n",
        "\n",
        "In this notebook, you will extract the MLFlow URI using the SDK. You will then use this information in future notebooks to integrate Axolotl with MLFlow.\n",
        "\n",
        "> Note: Retrieving the MLFlow URI is an operation that requires login to Azure. If you are running this notebook interactively, you can leverage the `DefaultAzureCredential()` method to authenticate. If you intend on running this code in a headless configuration (an AzureML component, or a serverless deployment) consider changing the authentication method to a non-interactive one. Alternative methods to retrieve your MLFlow URI can be found [on this link](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow-configure-tracking?view=azureml-api-2&tabs=python%2Cmlflow)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A| Finding your MLFlow URI\n",
        "\n",
        "In the previous notebook, you have learnt how to finetune an existing model (Phi1) on a dataset for a single epoch. The model that you've built was stored locally on disk in a folder called `output_dir` (the name of the folder differs depending on the configuration YML file you use).\n",
        "\n",
        "To properly track your experiments, you will instead need to leverage MLFlow. MLFlow enables you to do model tracking and registration, enabling you to use tracked models in downstream processes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import azureml.core\n",
        "workspace = azureml.core.Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1712640204568
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "mlflow_tracking_uri = ml_client.workspaces.get(ml_client.workspace_name).mlflow_tracking_uri\n",
        "print(mlflow_tracking_uri)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "azureml://australiaeast.api.azureml.ms/mlflow/v1.0/subscriptions/68092087-0161-4fb5-b51d-32f18ac56bf9/resourceGroups/aml-au/providers/Microsoft.MachineLearningServices/workspaces/aml-au\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1712640216039
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {}\n",
        "config[\"compute_size\"] = \"Standard_NC24ads_A100_v4\"\n",
        "config[\"compute_target\"] = \"a100cluster\"\n",
        "config[\"compute_node_count\"] = 1\n",
        "config[\"pytorch_configuration\"] = {\n",
        "    \"node_count\": 1, # num of computers in cluster\n",
        "    \"process_count\": 1} # gpus-per-computer * node_count\n",
        "\n",
        "config[\"experiment\"] = \"Finetune_phi1\"\n",
        "config[\"source_directory\"] = \"src\"\n",
        "config[\"environment\"] = \"axolotl_acpt\""
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1712640219153
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open the `src/Phi-ft.yml` file (which ships with Axolotl) and modify the mlflow value according to the [documentation](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/docs/config.qmd)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "with open('src/phi-ft.yml', 'r') as f:\n",
        "    phi_ft_config = yaml.safe_load(f)\n",
        "\n",
        "phi_ft_config[\"mlflow_tracking_uri\"] = mlflow_tracking_uri\n",
        "phi_ft_config[\"hf_mlflow_log_artifacts\"] = False\n",
        "phi_ft_config[\"mlflow_experiment_name\"] = config[\"experiment\"]\n",
        "\n",
        "with open('src/phi-ft-modified.yml', 'w') as f:\n",
        "    yaml.dump(phi_ft_config, f)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1712640227668
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note: The configuration below is different from the previous notebook. The training command now calls the newly created file."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "config[\"training_command\"] = \"accelerate launch -m axolotl.cli.train phi-ft-modified.yml\""
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1712640231750
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    cluster = azureml.core.compute.ComputeTarget(\n",
        "        workspace=workspace, \n",
        "        name=config['compute_target']\n",
        "    )\n",
        "    print('Found existing compute cluster')\n",
        "except azureml.core.compute_target.ComputeTargetException:\n",
        "    compute_config = azureml.core.compute.AmlCompute.provisioning_configuration(\n",
        "        vm_size=config['compute_size'],\n",
        "        max_nodes=config['compute_node_count']\n",
        "    )\n",
        "    cluster = azureml.core.compute.ComputeTarget.create(\n",
        "        workspace=workspace,\n",
        "        name=config['compute_target'], \n",
        "        provisioning_configuration=compute_config\n",
        "    )\n",
        "    \n",
        "cluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "InProgress..\nSucceededProvisioning operation finished, operation \"Succeeded\"\nSucceeded\nAmlCompute wait for completion finished\n\nMinimum number of nodes requested have been provisioned\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1712640245926
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = azureml.core.Experiment(workspace, config['experiment'])\n",
        "\n",
        "distributed_job_config = azureml.core.runconfig.PyTorchConfiguration(**config['pytorch_configuration'])\n",
        "aml_config = azureml.core.ScriptRunConfig(\n",
        "            source_directory=config['source_directory'],\n",
        "            command=config['training_command'],\n",
        "            environment=azureml.core.Environment.get(workspace, name=config[\"environment\"]),\n",
        "            compute_target=config['compute_target'],\n",
        "            distributed_job_config=distributed_job_config,\n",
        "    )\n",
        "run = experiment.submit(aml_config)\n",
        "run.set_tags({\n",
        "    \"environment\":config[\"environment\"],\n",
        "    \"epochs\": phi_ft_config[\"num_epochs\"],\n",
        "    \"micro_batch_size\": phi_ft_config[\"micro_batch_size\"],\n",
        "    \"sequence_len\": phi_ft_config[\"sequence_len\"],\n",
        "    \"dataset\": phi_ft_config[\"datasets\"][0][\"path\"]\n",
        "})\n",
        "\n",
        "print(f\"View run details:\\n{run.get_portal_url()}\")"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}