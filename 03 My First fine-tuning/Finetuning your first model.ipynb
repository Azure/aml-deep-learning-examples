{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning your first model\n",
    "\n",
    "Now that you've created your Docker environment and tested them, it is time for you to Finetune your first model.\n",
    "\n",
    "In this exercise, you will finetune a small model (Phi 1) on fictional data (garage-bAInd/Open-Platypus)\n",
    "\n",
    "## Goal\n",
    "\n",
    "Use Axolotl to finetune a small model (tiny-llama) on fictional data. To make the finetuning easier, you will not use Low Rank Adapters yet.\n",
    "\n",
    "The compute instances that you will use for this exercise contains only 1 GPU, and you will use only 1 node. To make the exercise as simple as possible, we will not introduce Distributed Training yet (although you have seen from a previous notebook how to submit the same command to multiple noeds in a cluster).\n",
    "\n",
    "## Introducing Axolotl\n",
    "\n",
    "Axolotl is a framework that easily allows you to pre-train and finetine a multitude of models by unifying the configuration in a standardized YAML file.\n",
    "\n",
    "The workflow for Axolotl is as follows:\n",
    "- Training: You perform training by running `accelerate launch -m axolotol.cli.train file.yaml`, where `file.yaml` is a yaml file that contains the Axolotl configuration. For more information about the structure and contents of this file, please refer to the Axolotl documentation.\n",
    "- Once your model is done training (depending on the configuration, model size, hardware size, data size), your model checkpoints will be stored locally on disk. To perform interactive inference on the newly pre-trained / finetuned model, run `accelerate launch -m axolotol.cli.inference file.yaml --model_dir=model_dir` where `model_dir` is the directory where Axolotl has saved the checkpoint. This directory is typically named `lora_out` when LoRA is used, or `model-out` when LoRA is not used. You can find the output directory in the YAML file as `output_dir`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Phi (1) on the Open Platypus dataset, without using PEFT, and without Distributed Training\n",
    "\n",
    "The Phi family of Small Language Models have been pre-trained on a mixed corpus containing filtered, curated web data that has been augmented with Synthetic data. The data used for training is predominantly focused on Mathematics (data similar to GSMK8+) and one programming (de-duped data from TheStack v3). More information about the training workflow for Phi can be found in the technical report \"Textbooks is all you need\".\n",
    "\n",
    "For our exercise, we will perform the following:\n",
    "1. We will create a single instance Standard_NC24ads_A100_v4 Virtual Machine. Please experiment with other VM sizes for your job, or alternatively proceed to the next notebook to experiment with Finetuning using LoRA / QLoRA.\n",
    "2. We will store an Axolotl YAML file in this repo, which we will use to configure the finetuning process\n",
    "3. We will finetune the model\n",
    "\n",
    "> Note: Although you will finetune the model, and that it will be saved on local disk - the model WILL BE DELETED UPON COMPLETION. In this notebook, you will not interacted with the fine-tuned model !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "workspace = azureml.core.Workspace.from_config()\n",
    "\n",
    "config = {}\n",
    "config[\"compute_size\"] = \"Standard_NC24ads_A100_v4\"\n",
    "config[\"compute_target\"] = \"a100cluster\"\n",
    "config[\"compute_node_count\"] = 1\n",
    "config[\"pytorch_configuration\"] = {\n",
    "    \"node_count\": 1, # num of computers in cluster\n",
    "    \"process_count\": 1} # gpus-per-computer * node_count\n",
    "config[\"training_command\"] = \"accelerate launch -m axolotl.cli.train phi-ft.yml\"\n",
    "config[\"experiment\"] = \"Finetune_phi1\"\n",
    "config[\"source_directory\"] = \"src\"\n",
    "config[\"environment\"] = \"axolotl_acpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute cluster\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cluster = azureml.core.compute.ComputeTarget(\n",
    "        workspace=workspace, \n",
    "        name=config['compute_target']\n",
    "    )\n",
    "    print('Found existing compute cluster')\n",
    "except azureml.core.compute_target.ComputeTargetException:\n",
    "    compute_config = azureml.core.compute.AmlCompute.provisioning_configuration(\n",
    "        vm_size=config['compute_size'],\n",
    "        max_nodes=config['compute_node_count']\n",
    "    )\n",
    "    cluster = azureml.core.compute.ComputeTarget.create(\n",
    "        workspace=workspace,\n",
    "        name=config['compute_target'], \n",
    "        provisioning_configuration=compute_config\n",
    "    )\n",
    "    \n",
    "cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting the finetuning request\n",
    "\n",
    "As with previous notebooks, you will create a new experiment, and submit a job. This time, we will perform it using only 1 of the two Docker images we've created.\n",
    "We will load the YAML file to extract some properties, that we will add as tags to our job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('src/phi-ft.yml', 'r') as f:\n",
    "    phi_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'garage-bAInd/Open-Platypus'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_config[\"datasets\"][0][\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting non-string tag to string: (epochs: 1)\n",
      "Converting non-string tag to string: (micro_batch_size: 2)\n",
      "Converting non-string tag to string: (sequence_len: 2048)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View run details:\n",
      "https://ml.azure.com/runs/Finetune_phi1_1712567210_561031dc?wsid=/subscriptions/68092087-0161-4fb5-b51d-32f18ac56bf9/resourcegroups/aml-au/workspaces/aml-au&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n"
     ]
    }
   ],
   "source": [
    "experiment = azureml.core.Experiment(workspace, config['experiment'])\n",
    "\n",
    "distributed_job_config = azureml.core.runconfig.PyTorchConfiguration(**config['pytorch_configuration'])\n",
    "aml_config = azureml.core.ScriptRunConfig(\n",
    "            source_directory=config['source_directory'],\n",
    "            command=config['training_command'],\n",
    "            environment=azureml.core.Environment.get(workspace, name=config[\"environment\"]),\n",
    "            compute_target=config['compute_target'],\n",
    "            distributed_job_config=distributed_job_config,\n",
    "    )\n",
    "run = experiment.submit(aml_config)\n",
    "run.set_tags({\n",
    "    \"environment\":config[\"environment\"],\n",
    "    \"epochs\": phi_config[\"num_epochs\"],\n",
    "    \"micro_batch_size\": phi_config[\"micro_batch_size\"],\n",
    "    \"sequence_len\": phi_config[\"sequence_len\"],\n",
    "    \"dataset\": phi_config[\"datasets\"][0][\"path\"]\n",
    "})\n",
    "\n",
    "print(f\"View run details:\\n{run.get_portal_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job succeeds after 35 minutes\n",
    "![Finetuning Phi1](img/ft_phi1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
