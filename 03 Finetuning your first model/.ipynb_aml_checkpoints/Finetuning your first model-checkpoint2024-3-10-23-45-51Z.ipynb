{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning your first model\n",
        "\n",
        "Now that you've created your Azure curated environment and tested it, it is time for you to Finetune your first model.\n",
        "\n",
        "In this exercise, you will finetune a small model (Phi 1) on fictional dataset (garage-bAInd/Open-Platypus). You will also integrate Axolotl with AzureML MLFlow, enabling you to retrieve logged metrics from Azure. You will save the output of your finetune exercise as part of the Azure experiment.\n",
        "\n",
        "## Goal\n",
        "\n",
        "Use Axolotl to finetune a small model on fictional data. To make the finetuning easier, you will not use Low Rank Adapters yet.\n",
        "\n",
        "The compute instances that you will use for this exercise contains only 1 GPU, and you will use only 1 node. To make the exercise as simple as possible, we will not introduce Distributed Training yet (although you have seen from a previous notebook how to submit the same command to multiple nodes in a cluster). Future notebooks will demonstrate how to run a GPU distributed job.\n",
        "\n",
        "## Introducing Axolotl\n",
        "\n",
        "Axolotl is a framework that easily allows you to pre-train and finetine a multitude of models by unifying the configuration in a standardized YAML file.\n",
        "\n",
        "The workflow for Axolotl is as follows:\n",
        "- Training: You perform training by running `accelerate launch -m axolotol.cli.train file.yaml`, where `file.yaml` is a yaml file that contains the Axolotl configuration. For more information about the structure and contents of this file, please refer to the Axolotl documentation.\n",
        "- Once your model is done training (depending on the configuration, model size, hardware size, data size), your model checkpoints will be stored locally on disk. To perform interactive inference on the newly pre-trained / finetuned model, run `accelerate launch -m axolotol.cli.inference file.yaml --model_dir=model_dir` where `model_dir` is the directory where Axolotl has saved the checkpoint. This directory is typically named `lora_out` when LoRA is used, or `model-out` when LoRA is not used. You can find the output directory in the YAML file as `output_dir`. Since you are submitting a job to AML, you will not interact with the finetuned model immediately once the job finishes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning Phi (1) on the Open Platypus dataset, without using PEFT, and without Distributed Training\n",
        "\n",
        "The Phi family of Small Language Models have been pre-trained on a mixed corpus containing filtered, curated web data that has been augmented with Synthetic data. The data used for training is predominantly focused on Mathematics (data similar to GSMK8+) and one programming (de-duped data from TheStack v3). More information about the training workflow for Phi can be found in the technical report \"Textbooks is all you need\".\n",
        "\n",
        "For our exercise, we will perform the following:\n",
        "1. We will create a single instance Standard_NC24ads_A100_v4 Virtual Machine. Please experiment with other VM sizes for your job, or alternatively proceed to the next notebook to experiment with Finetuning using LoRA / QLoRA.\n",
        "2. We will retrieve information about our workspace, more specifically our AzureML MLFlow URI, which we will insert in the configuration YML file. This way, we will be able to retrieve the learning metrics from the AzureML interface\n",
        "3. We will save the model's outputs to the folder `./outputs`, so that the finetuned model weights are saved permanently with the job submitted to AML.\n",
        "4. We will store an Axolotl YAML file in this repo, which we will use to configure the finetuning process. We will modify this file using the information from #2 and #3.\n",
        "5. We will finetune the model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Creating a target compute cluster to run our experiments"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "workspace = azureml.core.Workspace.from_config()\n",
        "\n",
        "config = {}\n",
        "config[\"compute_size\"] = \"Standard_NC24ads_A100_v4\"\n",
        "config[\"compute_target\"] = \"a100cluster\"\n",
        "config[\"compute_node_count\"] = 1\n",
        "config[\"pytorch_configuration\"] = {\n",
        "    \"node_count\": 1, # num of computers in cluster\n",
        "    \"process_count\": 1} # gpus-per-computer * node_count\n",
        "config[\"training_command\"] = \"accelerate launch -m axolotl.cli.train phi-ft.yml\"\n",
        "config[\"experiment\"] = \"Finetune_phi1\"\n",
        "config[\"source_directory\"] = \"src\"\n",
        "config[\"environment\"] = \"axolotl_acpt\""
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    cluster = azureml.core.compute.ComputeTarget(\n",
        "        workspace=workspace, \n",
        "        name=config['compute_target']\n",
        "    )\n",
        "    print('Found existing compute cluster')\n",
        "except azureml.core.compute_target.ComputeTargetException:\n",
        "    compute_config = azureml.core.compute.AmlCompute.provisioning_configuration(\n",
        "        vm_size=config['compute_size'],\n",
        "        max_nodes=config['compute_node_count']\n",
        "    )\n",
        "    cluster = azureml.core.compute.ComputeTarget.create(\n",
        "        workspace=workspace,\n",
        "        name=config['compute_target'], \n",
        "        provisioning_configuration=compute_config\n",
        "    )\n",
        "    \n",
        "cluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "InProgress..\nSucceededProvisioning operation finished, operation \"Succeeded\"\nSucceeded\nAmlCompute wait for completion finished\n\nMinimum number of nodes requested have been provisioned\n"
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2: Retrieve your MLFlow URI\n",
        "\n",
        "To properly track your experiments, you will instead need to leverage MLFlow. MLFlow enables you to do model tracking and registration, enabling you to use tracked models in downstream processes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
        "mlflow_tracking_uri = ml_client.workspaces.get(ml_client.workspace_name).mlflow_tracking_uri\n",
        "print(mlflow_tracking_uri)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "azureml://australiaeast.api.azureml.ms/mlflow/v1.0/subscriptions/68092087-0161-4fb5-b51d-32f18ac56bf9/resourceGroups/aml-au/providers/Microsoft.MachineLearningServices/workspaces/aml-au\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1712738695917
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(\"/config.json\", 'r') as f:\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 and 4. Load the Phi-1 Axolotl YML file\n",
        "\n",
        "In the code snipped below, you will:\n",
        "\n",
        "1. Load an existing YML file that ships with Axolotl. For other examples, please review their repository.\n",
        "2. You will use the MLFLow URI from above to insert new settings in the loaded dictionary.\n",
        "3. You will change the `output_dir` value to `./outputs`. This folder on AzureML enables the persistence of output data.\n",
        "4. You will save the configuration to a new file called `phi-ft-modified.yml`\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "with open('src/phi-ft.yml', 'r') as f:\n",
        "    phi_ft_config = yaml.safe_load(f)\n",
        "\n",
        "phi_ft_config[\"mlflow_tracking_uri\"] = mlflow_tracking_uri\n",
        "phi_ft_config[\"hf_mlflow_log_artifacts\"] = False\n",
        "phi_ft_config[\"mlflow_experiment_name\"] = config[\"experiment\"]\n",
        "phi_ft_config[\"output_dir\"] = \"./outputs\"\n",
        "\n",
        "with open('src/phi-ft-modified.yml', 'w') as f:\n",
        "    yaml.dump(phi_ft_config, f)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Submitting the finetuning request\n",
        "\n",
        "As with previous notebooks, you will create a new experiment, and submit a job. This time, we will perform it using only 1 of the two Docker images we've created.\n",
        "We will load the YAML file to extract some properties, that we will add as tags to our job"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = azureml.core.Experiment(workspace, config['experiment'])\n",
        "\n",
        "distributed_job_config = azureml.core.runconfig.PyTorchConfiguration(**config['pytorch_configuration'])\n",
        "aml_config = azureml.core.ScriptRunConfig(\n",
        "            source_directory=config['source_directory'],\n",
        "            command=config['training_command'],\n",
        "            environment=azureml.core.Environment.get(workspace, name=config[\"environment\"]),\n",
        "            compute_target=config['compute_target'],\n",
        "            distributed_job_config=distributed_job_config,\n",
        "    )\n",
        "run = experiment.submit(aml_config)\n",
        "run.set_tags({\n",
        "    \"environment\":config[\"environment\"],\n",
        "    \"epochs\": phi_ft_config[\"num_epochs\"],\n",
        "    \"micro_batch_size\": phi_ft_config[\"micro_batch_size\"],\n",
        "    \"sequence_len\": phi_ft_config[\"sequence_len\"],\n",
        "    \"dataset\": phi_ft_config[\"datasets\"][0][\"path\"]\n",
        "})\n",
        "\n",
        "print(f\"View run details:\\n{run.get_portal_url()}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Converting non-string tag to string: (epochs: 1)\nConverting non-string tag to string: (micro_batch_size: 2)\nConverting non-string tag to string: (sequence_len: 2048)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "View run details:\nhttps://ml.azure.com/runs/Finetune_phi1_1712713399_fc32d6dd?wsid=/subscriptions/68092087-0161-4fb5-b51d-32f18ac56bf9/resourcegroups/aml-au/workspaces/aml-au&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n"
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The job succeeds after around 35 minutes. Notice the std_out display on the right pane, and the contents of the `./outputs` directory, showing the persisted Model checkpoints.\n",
        "![Finetuning Phi1](img/phi_ft_1.png)\n",
        "\n",
        "the integration with MLFlow can also be shown by observing the `metrics` tab:\n",
        "![MLFlow integration](img/phi_ft_2.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}